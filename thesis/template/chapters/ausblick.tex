\chapter{Zusammenfassung und Ausblick}
\label{chapter:ausblick}

Diese Kapitel fasst die Ergebnisse der Arbeit basierend auf den Teilergebnissen der vorigen Kapitel zusammen und liefert einen Ausblick über mögliche Folgethemen, die im Laufe dieser Arbeit herausgearbeitet wurden. 

\section{Zusammenfassung}

Dieser Abschnitt bietet eine Zusammenfassung der einzelnen Kapitel dar und stellt eine Verbindung der Ergebnisse aller Kapitel her. Zu Beginn dieser Arbeit zunächst in Kapitel \ref{chapter:association} die Optimierung einer fertigen TSDF-Karte mit initialer Pose-Historie algorithmisch analysiert. Dazu wurde untersucht, inwiefern Assoziationen zwischen dem initialen Pose-Graphen und der gegebenen TSDF-Karte hergestellt werden können. Dazu wurde im ersten Schritt herausgestellt, dass anhand der TSDF-Karte ohne zusätzliche Informationen keine gute Assoziationsbestimmung möglich ist. Wird die Karte zusätzlich für jede Zelle um eine Historie erweitert, die aufzeigt, von welchen Posen ausgehend die entsprechende Zelle beschrieben wird, relativiert sich dieses Problem. Dies sorgt wie herausgestellt allerdings für eine starke Vergrößerung des für die Karte benötigten Speichers. Zusätzlich stellt dies Probleme für die HDF5-Datenstruktur dar. Aus diesem Grund wurde die Ermittlung von Assoziationen mittels eines Raytracers untersucht, der ausgehend von den einzelnen Posen Strahlen aussendet und mit dem TSDF schneidet und entsprechend geschnittene Zellen mit der Pose assoziiert. Diese Bestimmung der Assoziationen funktioniert im Grundsatz, eröffnet allerdings weitere Probleme wie Karten-Bereiche die durch Verdeckungen nicht assoziiert werden können. Dazu gehören unter anderem die Teile der Karte, die durch die Optimierung aus der Karte herausgelöscht werden sollen, wie zum Beispiel doppelte Wände. Dadurch prägte sich heraus, dass der gewählte Prozess von großen Unsicherheiten geprägt ist. Dies verstärkte sich zusätzlich durch Probleme bei der Identifikation von Schleifenschlüssen, da die Datenbasis, auf Basis der Schleifenschlüsse identifiziert und validiert werden, hier die TSDF-Karte, zum Zeitpunkt der Identifikation bereits fehlerhaft ist. Dies sorgt implizit für fehlerhafte Schleifenschlüsse. Aus diesen Grund wurde der zunächst gewählte Ansatz verworfen und stattdessen auf einer größeren Datenbasis gearbeitet, die eine initiale Pose-Schätzung und die zugehörigen Punktwolken erhält. Ziel die Implementation und Konzeption eines Ansatzes, der in ein SLAM Verfahren wie den Ansatz von Eisoldt et al. \cite{HATSDF} integriert werden kann und zur Laufzeit die zur Registrierung neuer Daten genutzt TSDF-Karte auf Basis identifizierter Schleifenschlüsse zu optimieren. 

Um dieses Ziel zu erreichen wurde zunächst in Kapitel \ref{chapter:loop_closure} die Detektion von Schleifenschlüssen und die Optimierung des Pose-Graphen basierend auf der neuen Datenbasis behandelt. Dazu wird mittels eines \emph{k-d Baumes}, der die 3-D Position jeder Pose des Pose-Graphen enthält, für die aktuell betrachtete Pose eine \emph{Radius-Suche} mit einem parametrisierten Radius durchgeführt. Dies liefert eine Reihe von Kandidaten, die über ein \emph{Scan-Matching} zwischen den Punktwolken der beiden beteiligten Posen validiert werden. Dies liefert bereits gute Ergebnisse, jedoch ist die Anzahl so detektierter Schleifenschlüsse gering. Zur Verbesserung wurde auf der Annahme lokaler Konsistenz ein Fenster eingeführt, welches die (Model-)Punktwolke der Kandidaten-Pose mit den Punktwolken der umgebenden Posen anreichert beziehungsweise verbindet. Dies sorgt für eine deutlich dichtere Model-Punktwolke und damit verbunden für eine bessere Konvergenz des Scan-Matching und eine größere Anzahl validierter Schleifenschlüsse. Trotz der bereits vorgenommenen Validierung wurden Sonderfälle identifiziert, bei denen fehlerhafte Schleifenschlüsse validiert wurden. Dies tritt besonders in Umgebungen auf, in denen vergleichsweise wenig Features vorhanden sind, wie zum Beispiel in Fluren. Aus diesen Grund wurden zwei weitere Validatoren implementiert, die eine zusätzliche Beschränkung für Schleifenschlüsse darstellen. Sie stellen sicher, dass die Schleifenschlüsse sich innerhalb eines statischen Rahmens, gegeben durch die Unsicherheiten des Faktor-Graphen, der für die Optimierung des Pose-Graphen verwendet wird, bewegen. Dies erhöht die Robustheit dieses Ansatz enorm, da fehlerhafte Schleifenschlüsse zu großen Problemen innerhalb der Optimierung sorgen. Hier wird die Implementation des Faktor-Graphen von GTSAM \ref{dellaert2012factor} mitsamt der zugehörigen Optimierungsfunktionen verwendete. Dies sorgt für eine Optimierung des Pose-Graphen anhand von Schleifenschlüssen, die sowohl die Validierung durch das Scan-Matching, als auch die beiden weiteren vorgestellten Validierungsstufen durchlaufen haben. Insgesamt liefert der Ansatz sehr gute und mit anderen Herangehensweisen an dieses Optimierungsproblem wie zum Beispiel ELCH \cite{sprickerhof2011heuristic} vergleichbare Ergebnisse. 

Basierend auf diesen Ergebnissen der Pose-Graph Optimierung wurde in Kapitel \ref{chapter:map_update} die Optimierung der TSDF-Karte erneut beleuchtet. Dazu wurden zwei Möglichkeiten des Kartenupdates erläutert und miteinander verglichen. Das globale Update löscht die gesamte Karte nach Identifikation eines oder mehrerer Schleifenschlüsse und der resultierenden Anpassung des Pose-Graph. Nachdem die Karte gelöscht ist, wird sie vollständig neu generiert, indem inkrementelle TSDF-Updates basierend auf den neuen Posen und zugehörigen Punktwolken durchgeführt werden. Dieses globale Update ist allerdings sehr zeitintensiv und es werden jeweils Teile der Karte gelöscht, die keiner Löschung bedürfen, weil die mit den Teilbereichen assoziierten Posen nicht, oder nur minimal verschoben wurden. Aus diesem Grund wurde ein partielles Update entwickelt, welches nur die betroffenen Teilbereiche löscht. Zum Vergleich dient das globale Update als Base-Line. Um das partielle Update zu realisieren wurde zunächst das Tupel jeder TSDF-Zelle um ein weiteres Feld erweitert, welches den Index der Pose enthält, die zuletzt in diese Zelle geschrieben hat. Basierend auf dieser neuen Karten-Struktur wurden zwei Methoden vorgestellt, das Update durchzuführen. Dazu wurde die Teilmenge der Posen des Pose-Graphen bestimmt, deren Veränderung durch die Optimierung des Pose-Graphen größer ist als eine definierte Schwelle. Daraufhin werden alle Zellen, deren Pose-Index in der bestimmten Teilmenge der Posen enthalten ist, mit Default-Werten überschrieben. Hierfür wurden zwei Methoden entwickelt, wobei performantere Methode die Abstraktion der lokalen Karte umgeht und lediglich die Strukturen der globalen Karte verwendete. Im Anschluss wird die Karte per inkrementellem Update neu gefüllt. Dies sorgt für eine deutliche Reduktion der Laufzeit des Algorithmus. Im Falle vom Hannover-1 Datensatz benötigt das partielle Update nur $15\%$ der Laufzeit des globalen Updates. Dabei ist eine leichte Verringerung der Genauigkeit zu beobachten, die allerdings im Schnitt nicht größer ist als die Auflösung der lokalen Karte, die der begrenzende Faktor für die Genauigkeit des Ergebnisses ist.

\improvement{ggf. über die Place recognition von Canelhas schreiben: \cite{Canelhas2017TruncatedSD}}

Zusammenfassend lässt sich sagen, dass der vorgestellte Ansatz zur Optimierung des Pose-Graphen in Kombination mit dem partiellen Update der TSDF-Karte gute Ergebnisse liefert und die initiale Schätzungen -- und damit verbunden auch die Karte -- deutlich verbessert. Bezogen auf die Laufzeit ist an dieser Stelle allerdings noch viel Optimierungspotential und der Ansatz nicht näherungsweise Echtzeit fähig. Zusätzlich ist die verwendete Kartenstruktur mitsamt der Kapselung über die lokale Karte nicht optimal für den vorgestellten Ansatz, da eine Vielzahl von Operationen durchgeführt werden muss, die sehr zeitintensiv sind. Im Folgenden Abschnitt wird ein Ausblick über Themen gegeben, die nicht in den zeitlichen oder thematischen Rahmen gepasst haben, aber für zukünftige Forschungen in diesem Bereich von Interesse sein könnten.

\section{Ausblick}

Dieser Abschnitt stellt mögliche zukünftige Arbeiten heraus, die auf den hier vorgestelltem Themenschwerpunkt aufbauen. Dabei wird an dieser Stelle nur auf inhaltliche Themen eingegangen. Im folgenden Absatz wird gekapselt davon auf mögliche Optimierungen der implementierten Lösung eingegangen, insbesondere im Bereich der Laufzeitoptimierung.

Ein Teil dieser Arbeit, der weiterer Evaluation und Arbeit bedarf sind die im Kapitel \ref{chapter:loop_closure} vorgestellten Validatoren für Schleifenschlüsse, die dem Scan-Matching nachgestellt sind und so eine weitere Validationsebene darstellen. Diese zusätzliche Validation ist essentiell, da fehlerhafte Schleifenschlüsse die Ergebnisse der Faktorgraph-Optimierung merklich verschlechtern. Die beiden Validatoren wurden innerhalb dieser Arbeit als Reaktion auf die Identifikation fehlerhafter Schleifenschlüsse entwickelt und erfüllen grundsätzlich ihren Zweck. An dieser Stelle wurde allerdings nicht evaluiert ob die Validatoren in der Lage sind, jegliche fehlerhafte Schleifenschlüsse zu identifizieren. Zusätzlich gilt es zu analysieren, welche anderen Lösungen für diese zusätzliche Evaluation existieren. Grundlage hierfür könnten die Ausführungen von Xie et al. bieten, die ebenfalls eine Zwischen-Ebene zur Validierung identifizierter Schleifenschlüsse bieten um die Robustheit des SLAM-Algorithmus zu verbessern.
Ein weiterer Bereich des hier vorgestellten Ansatzes, der optimiert werden kann, ist das Scan-Matching, welches sowohl für die Vorregistrierung neuer Daten als auch für die erste Validierungsstufe bei der Schleifenschluss-Detektion verwendet wird. Je besser die Ergebnisse des Scan-Matching, desto robuster das SLAM-Verfahren. In dieser Arbeit wird an beiden Stellen GICP verwendet. Es wurden zusätzlich allerdings bereits Versuche unternommen die Bibliothek \emph{TEASER++} \cite{Yang20tro-teaser} in diesen Ansatz zu integrieren, um das Scan-Matching basierend auf zuvor berechneten Korrespondenzen zu verbessern. Bei der Integration wurde allerdings ein Fehler in der von TEASER++ zur Verfügung gestellten API festgestellt. Aufgrund dieses Fehlers wurde am GitHub-Repository der Bibliothek ein Bug-Ticket erstellt und die Integration der Bibliothek vorerst hinten an gestellt. Vor Kurzem gab es nun Aktivität am erstellten Ticket und der Fehler scheint behoben. In einer zukünftigen Arbeit lohnt es sich gegebenenfalls, an dieser Stelle anzusetzen und Teaser++ zu integrieren.
Auch im Bereich des Karten-Updates sind mehrere Optimierungen, insbesondere bezogen auf die Laufzeit, möglich. An dieser Stelle wird allerdings ein experimenteller Ansatz eruiert, dessen Evaluation sich in zukünftigen Arbeiten bezahlt machen könnte. Kapitel \ref{chapter:map_update} stellt eine Methode vor, mit deren Hilfe ein partielles Update durch die Speicherung des Pose-Indices in einer Zelle der TSDF-Karte, die diese zuletzt bearbeitet hat, möglich ist. Diese Methode könnte an dieser Stelle verwendet werden um einige der in Kapitel \ref{chapter:association} identifizierten Probleme bei der Generation von Datenassoziationen zu lösen. Dies betrifft insbesondere Verdeckungen. Trifft ein Strahl des Raytracers eine TSDF-Zelle können direkt mehr Informationen abgelesen werden. So kann ein simulierter Scan, ausgesendet von einer Pose $P_i$ bedenkenlos durch Wände hindurch schauen, wenn die Pose-Indices der Zellen der Wand niedriger sind als der Index der Pose selbst. Dadurch können zum Beispiel falsch platzierte Wände nun trotz der räumlichen Verdeckung innerhalb des TSDF identifiziert werden. An dieser Stelle lohnt sich eine Evaluation dieses neuen Ansatzes unter Berücksichtigung der weiteren, in Kapitel \ref{chapter:association} dargelegten Problemstellungen. Lässt sich durch diesen Ansatz ebenfalls das Problem der Identifikation von Schleifenschlüssen basierend auf einer TSDF-Karte lösen, könnte die in dieser Arbeit um die Punktwolken erweiterte Datenbasis möglicherweise wieder um diese reduziert werden. 

Zu Beginn dieser Arbeit lag der Fokus vornehmlich auf der Identifikation von Assoziationen auf Basis einer initialen Schätzung innerhalb einer fertigen TSDF-Karte. Allerdings existierten zu diesem Zeitpunkt lediglich fertige TSDF-Karten ohne zugehörige initiale Schätzung der Pose-Historie. Aus diesem Grund wurde eine naive Pfad-Exploration implementiert, die innerhalb einer fertigen Karte, gegeben einen Startpunkt $S_i$, eine Schätzung des abgelaufenen Pfades generiert und abspeichert. Auch an dieser Stelle wurde der implementierte Laserscanner verwendet. Für die Pfadschätzung wurden zur Vereinfachung als mögliche Posen nur die Ursprungspositionen der Chunks der globalen Karte verwendet. Dazu wurde zunächst die Anzahl der betrachteten Chunks reduziert, indem für jeden Chunk eine Abschätzung darüber getroffen wird, ob dieser dafür in Frage kommt. Dies ist der Fall, wenn alle Chunks um den betrachten Chunk herum, in einem Bereich identischer Größe zu der der lokalen Karte initialisiert wurden. Dies liegt daran, dass alle Chunks, die sich zu einem beliebigem Zeitpunkt innerhalb der lokalen Karte befinden, bereits initialisiert sind und mit Default-Werten befüllt werden. Gilt dies für den betrachteten Chunk nicht, heißt das implizit, dass das Zentrum der lokalen Karte sich nie in der Nähe dieses Chunks befunden hat. Dieser Chunk kommt entsprechend nicht für die Exploration in Frage. Für alle in Frage kommenden Chunks wird im Anschluss mittels des Raytracers ein Scan simuliert und das Verhältnis zwischen der Anzahl von Strahlen gebildet, die eine Datenassoziation herstellen mit denen, die lediglich leeren Raum passieren. Dies liefert für die in Frage kommenden Chunks ein Raster, welches die zuvor berechneten Verhältnisse enthält. Innerhalb dieses Rasters kann nun mit einer Such-Methode wie zum Beispiel einer abgewandelten Variante des Dijkstra Algorithmus für kürzeste Pfade \cite{noto2000method} ein Pfad generiert werden. Diese Variante wurde an dieser Stelle rein experimentell verwendet und nicht weiter ausgearbeitet. Es konnten allerdings Pfade generiert werden, die -- gegeben die aktuell betrachtete Karte -- als mögliche Pfade durch die Karte angesehen werden können und gegebenenfalls für Roboter-Trajektorien durch bekannte Umgebungen in Frage kommen. An dieser Stelle lohnt sich eine weitere Betrachtung in Hinsicht auf die Verwendbarkeit des Ansatzes zur Identifikation von Pfaden innerhalb einer gegebenen TSDF-Karte.

Im Folgenden wird ein Ausblick auf zeitliche Optimierungen des vorgestellten Ansatzes zur Kartenoptimierung nach der Detektion von Schleifenschlüssen gegeben.´

\subsection{Optimierungen des vorgestellten Ansatzes}

Wie in Kapitel \ref{chapter:map_update} herausgestellt ist, ist die vorhandene Karten-Repräsentation, besonders durch die Abstraktion der lokalen Karte, nicht gut für das Szenario des Schleifenschlusses und der damit verbundenen Graph-Optimierung geeignet. Hauptgrund ist die benötigte Durchführung mehrerer laufzeitintensiver Operationen um die Karte in einen konsistenten Zustand überführen zu können. Für das TSDF-Update haben Eisoldt et al. \cite{HATSDF} bereits für eine kleine lokale Karte gezeigt, dass eine Optimierung der Laufzeit vom Sekundenbereich in den Bereich weniger Millisekunden durch die Nutzung eines \emph{Field Programmable Gate Array (FPGA)} möglich ist. Dies wurde allerdings für den \emph{Shift} der lokalen Karte, der ebenfalls einen großen Teil der Laufzeit des hier vorgestellten Ansatzes zum Karten-Update ausmacht, nicht weiter verfolgt. Ein Umgehen der Abstraktion der lokalen Karte konnte an dieser Stelle beim Zurücksetzen der vom Update betroffenen TSDF-Zellen konnte an dieser Stelle bereits eine deutliche Reduktion der Laufzeit erwirken, da dadurch die Anzahl benötigter Shifts um die Hälfte reduziert werden konnte. Für das Schreiben der neuen Zellen ist allerdings bei der gegebenen Struktur der Karte die Durchführung mehrerer Shifts notwendig. Um dieses Problem zu lösen gibt es mehrere Lösungsansätze, die im folgenden unterbracht werden. Eine Möglichkeit besteht in der Konzeption einer Datenstruktur für die Karte, die besser auf die Anforderungen des Karten-Updates ausgerichtet ist und den Datenaustausch mit der Festplatte auf ein Minimum reduziert. Ergibt sich an dieser Stelle keine Variante, die die Anforderungen abbilden kann, sollte der Fokus auf der Optimierung der bereits bestehenden Strukturen liegen. Eine weitere Optimierung des TSDF-Update -- zum Beispiel durch die Nutzung von Grafikprozessoren -- und eine initiale Optimierung des Shifts könnten hier erste Schritte darstellen. Beide Operationen sind für den Großteil der Laufzeit des partiellen Updates der Karte verantwortlich. Die TSDF-Updates benötigen in etwa $65\%$ der Zeit, die Shifts der Karte ungefähr $35\%$. Aus diesem Grund sind beide Operationen die \emph{Bottlenecks} des Karten-Updates und erfordern eine tiefere Betrachtung. Bereits in vorigen Betrachtungen des Shifts wurde festgestellt, dass eine Parallelisierung zwar möglich, aber sehr komplex ist. Hier ist abzuwägen, ob der Aufwand für diese Parallelisierung gerechtfertigt ist, oder ein Ansatz gewählt wird, der sich vollständig vom Aktuellen unterscheidet. In jedem Fall ist hier durch eine Parallelisierung mittels FPGAs oder Grafikprozessoren eine erhebliche Reduktion des zeitlichen Aufwandes zu erwarten. Es bleibt zu evaluieren, ob durch die Optimierungen eine Integrierung dieses Ansatzes in einen Echtzeit SLAM Ansatz möglich ist, oder ob gegebenenfalls das Update der Karte parallel zum Rest des SLAM-Ansatzes geschaltet wird. Neben der Optimierung durch eine Beschleunigung der bestehenden Operationen ist auch eine Optimierung durch eine Anpassung der Algorithmik für den Karten-Shift denkbar. Dies wurde bereits in Kapitel \ref{chapter:map_update} kurz behandelt. Derzeit wird vor jedem Schritt des Updates ein Shift in Richtung der zugehörigen Pose durchgeführt. Dies ist unter gewissen Umständen nicht immer nötigt. Stattdessen kann hier eine andere Strategie genutzt werden, die die lokale Karte nur dann verschiebt, wenn es notwendig ist.

Die genannten Punkte liefern nur einen ausgewählten Überblick der möglichen Themen, die basierend auf diesem Ansatz weiter behandelt werden können. Dazu kann der hier vorgestellte Ansatz als Grundlage oder Base-Line für weitere Forschungen dienen.
